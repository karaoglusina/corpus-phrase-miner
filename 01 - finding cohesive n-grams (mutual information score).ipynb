{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 1 - finding cohesive n-grams (MI/PMI)\n",
        "\n",
        "An n-gram is a sequence of n consecutive words. In our analysis:\n",
        "\n",
        "- **Bigram** (n=2): \"opportunity areas\"\n",
        "- **Trigram** (n=3): \"body and mind\"\n",
        "\n",
        "These sequences can capture concepts that individual words miss. \"Status\" and \"symbol\" mean different things separately than \"status symbol\" does together.\n",
        "\n",
        "In this notebook, we create a comprehensive set of bigrams/trigrams from the transcript and compute their **association strength** (MI), with **length-wise normalization** and **speaker adoption**, to prepare candidates for downstream filtering. \n",
        "\n",
        "\n",
        "Please refer to the README.md for an overview of the whole analysis.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Inputs & Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### create environment and install dependencies\n",
        "\n",
        "This project is tested with **Python 3.9.6**. Some dependencies (e.g., VegaFusion/Altair integrations) work best on this version. To make sure everything runs smoothly,\n",
        "\n",
        "→ Open a terminal (macOs: press `⌘ + Space`, type 'Terminal', hit Enter | windows: press `Win + R`, type 'Command Prompt', hit Enter)  \n",
        "→ Paste the below commands (replace the path in first command with the folder.) Correct python version and all dependencies will be installed automatically.\n",
        "\n",
        "```bash\n",
        "# Go to the project folder\n",
        "cd /project_path\n",
        "# Create the environment and install all dependencies:\n",
        "conda env create -f environment.yml\n",
        "# Activate the environment:\n",
        "conda activate vis-analysis\n",
        "# Register the environment in Jupyter so that you can select it inside notebooks:\n",
        "python -m ipykernel install --user --name vis-analysis --display-name \"Python 3.9.6 (vis-analysis)\"\n",
        "```\n",
        "→ When opening a notebook, select the kernel called “Python 3.9.6 (vis-analysis)” from the Jupyter or VS Code kernel menu.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### import the functions and modules\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core imports\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import altair as alt\n",
        "from IPython.display import display\n",
        "\n",
        "alt.data_transformers.enable(\"vegafusion\")\n",
        "\n",
        "os.makedirs('altair', exist_ok=True)\n",
        "\n",
        "# Display options\n",
        "pd.set_option('display.max_rows', 10)\n",
        "pd.set_option('display.max_columns', 50)\n",
        "sns.set_context('talk')\n",
        "\n",
        "# Output folder\n",
        "os.makedirs('outputs', exist_ok=True)\n",
        "\n",
        "# Optional - Enable automatic reloading of modules when source code changes. This eliminates the need to restart the kernel when updating external .py files.\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load and prepare design conversation transcripts\n",
        "\n",
        "The transcript should be a table of utterances with at least the columns:  \n",
        "- `speaker`  \n",
        "- `text`  \n",
        "- (optional) `session`, if multiple sessions are included  \n",
        "\n",
        "We also apply basic cleaning:  \n",
        "- Remove transcription artifacts (e.g., `[inaudible]`)  \n",
        "- Standardize case and punctuation  \n",
        "- Merge consecutive utterances from the same speaker into larger “turns” so that multiword phrases spanning lines are not missed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ======================================\n",
        "# OPTION 1: Load your own data\n",
        "# ======================================\n",
        "# Replace 'your_data.csv' with your actual data file\n",
        "# Your CSV should have columns: 'text' (required), 'speaker' (optional), 'session' (optional)\n",
        "\n",
        "from src.text_utils import clean_text, utterances_to_turns\n",
        "\n",
        "# Example loading your data:\n",
        "# all_utterances = pd.read_csv('data/your_corpus.csv')\n",
        "# cleaned = clean_text(all_utterances)\n",
        "# session_turns = utterances_to_turns(cleaned)\n",
        "\n",
        "# ======================================\n",
        "# OPTION 2: Use sample data for demonstration\n",
        "# ======================================\n",
        "# Creating a small sample dataset to demonstrate the analysis\n",
        "\n",
        "sample_data = pd.DataFrame({\n",
        "    'speaker': ['Alice', 'Bob', 'Alice', 'Bob', 'Charlie', 'Alice', 'Bob', 'Charlie'] * 10,\n",
        "    'text': [\n",
        "        'We need to think about user experience and interaction design.',\n",
        "        'Yes, the user interface should be intuitive and clean.',\n",
        "        'What about incorporating machine learning algorithms?',\n",
        "        'Good idea. Machine learning could help with personalization.',\n",
        "        'We should also consider data privacy and security concerns.',\n",
        "        'Absolutely. Privacy by design is essential.',\n",
        "        'Let me check the technical requirements for implementation.',\n",
        "        'The system architecture needs to be scalable.'\n",
        "    ] * 10,\n",
        "    'session': [1, 1, 1, 1, 1, 1, 1, 1] * 10\n",
        "})\n",
        "\n",
        "# Process the data\n",
        "cleaned = clean_text(sample_data)\n",
        "session_turns = utterances_to_turns(cleaned)\n",
        "\n",
        "print(f'Total turns: {len(session_turns)}')\n",
        "print(f'Unique speakers: {session_turns[\"speaker\"].nunique()}')\n",
        "print(f'Sessions: {session_turns[\"session\"].nunique() if \"session\" in session_turns.columns else \"N/A\"}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Extract bigrams and trigrams with frequency counts\n",
        "We start by extracting all contiguous bigrams and trigrams from the corpus. This gives us our raw material.  Most are unremarkable (\"and the\", \"I think\"), but some capture how the team thinks about the problem. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We extract **n-grams** (sequences of n consecutive words) from each speaker turn.  \n",
        "- **Unigrams** = single words\n",
        "- **Bigrams** = 2 words in sequence  \n",
        "- **Trigrams** = 3 words in sequence  \n",
        "\n",
        "This gives us candidate phrases to later evaluate for **cohesion strength**.  \n",
        "At this stage, we only count surface forms; filtering and statistical measures come next."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.ngram_extraction import extract_ngrams_from_corpus, count_speakers_per_ngram\n",
        "\n",
        "session_ngrams = extract_ngrams_from_corpus(\n",
        "    session_turns,\n",
        "    text_column='text',\n",
        "    frequency_column='frequency_in_session',\n",
        "    n_gram_min=1,\n",
        "    n_gram_max=3,\n",
        "    preprocess=True,\n",
        "    remove_punctuation=True\n",
        ")\n",
        "\n",
        "session_ngrams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Understanding the frequency distribution of ngrams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1. Histograms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before exploring the long table, let’s plot histograms to see how n-gram frequencies are distributed, so we get a sense of which frequency ranges to inspect for the special n-grams we’re looking for."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.mi_visualization import create_log_binned_histogram_linear, create_log_binned_histogram_normalized\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Filter dataframes for different n-gram lengths\n",
        "unigram_df = session_ngrams[session_ngrams['ngram_length'] == 1]\n",
        "bigram_df = session_ngrams[session_ngrams['ngram_length'] == 2]\n",
        "trigram_df = session_ngrams[session_ngrams['ngram_length'] == 3]\n",
        "\n",
        "# Create a list of dataframes and titles\n",
        "ngram_dfs = [trigram_df, bigram_df, unigram_df]\n",
        "titles = ['Trigrams', 'Bigrams', 'Unigrams']\n",
        "\n",
        "# Create and display the histograms with linear y-axis\n",
        "fig = create_log_binned_histogram_linear(ngram_dfs, titles, frequency_column='frequency_in_session', n_bins=8, figsize=(12, 8))\n",
        "plt.show()\n",
        "\n",
        "# Create and display the histograms with normalized y-axis\n",
        "fig = create_log_binned_histogram_normalized(ngram_dfs, titles, frequency_column='frequency_in_session', n_bins=8, figsize=(12, 8))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each bar counts how many distinct n-grams fall within a given frequency range on the x-axis. The ranges are logarithmically spaced (shared across Unigram/Bigram/Trigram panels), so the subplots are directly comparable.\n",
        "\n",
        "We used log-spaced bins on the x-axis to stop everything from collapsing into the first bin and make both the high-frequency head and the long tail visible.\n",
        "\n",
        "**Linear y-axis view (first figure)** Uses raw counts on the y-axis. This shows how extremely skewed the data. The leftmost (low-frequency) bins dominate, while high-frequency bins are barely visible.\n",
        "\n",
        "**Normalized y-axis view (second figure)** rescales the counts so we can more easily compare shapes across n-gram lengths without the sheer size of the rare bin overwhelming everything.\n",
        "\n",
        "**Comparing n-gram lengths**\n",
        "As n increases, mass shifts left (toward lower frequencies). Unigrams retain more high-frequency items; bigrams fewer; trigrams are overwhelmingly rare.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2. Using frequency rank instead of raw frequency (Zipf plot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Histograms are hard to read when the distribution is this skewed: almost everything piles into the first bin. This is because language follows a predictable pattern called Zipf's Law:\n",
        "\n",
        "- A small number of phrases account for most usage\n",
        "- The vast majority of phrases are rare\n",
        "\n",
        "While searching for a range that frame-signaling n-grams appear, a Zipf plot can be more useful. This plot replaces raw counts with ranks (the 1st most common, 2nd, ...nth) and uses log–log axes, so the head and the long tail become readable on one scale. \n",
        "\n",
        "_Hover on the points to see the n-grams..._\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.mi_visualization import add_frequency_rank\n",
        "# Global rank → write to 'frequency_rank_global'\n",
        "session_ngrams = add_frequency_rank(session_ngrams, freq_col='frequency_in_session', method='sequential', rank_col='frequency_rank_global')\n",
        "# Rank within ngram_length → write to 'frequency_rank_within_length'\n",
        "session_ngrams = add_frequency_rank(session_ngrams, freq_col='frequency_in_session', group_by='ngram_length', method='sequential', rank_col='frequency_rank_within_length')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.mi_visualization import create_parametric_ngram_scatter, normalise_values\n",
        "session_ngrams = normalise_values(\n",
        "    session_ngrams,\n",
        "    freq_col='frequency_in_session',\n",
        "    out_col='normalized_frequency_within_length',\n",
        "    method='max',\n",
        "    group_by='ngram_length'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.mi_visualization import create_parametric_ngram_scatter\n",
        "\n",
        "chart = create_parametric_ngram_scatter(\n",
        "    df = session_ngrams,\n",
        "    x_col='frequency_rank_within_length', x_mode='continuous', x_scale='log', x_bin=False,\n",
        "    y_col='normalized_frequency_within_length',     y_mode='continuous', y_scale='log', y_bin=False,\n",
        "    color_col='ngram_length', color_mode='categorical',\n",
        "    color_scheme='tableau10',\n",
        "    width=600, height=450\n",
        ")\n",
        "chart"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each point in this plot is an n-gram. Colors separate Unigrams / Bigrams / Trigrams.  \n",
        "x = global rank,   \n",
        "y = frequency normalized to the single global maximum (both axes log-scaled).\n",
        "\n",
        "- Left/top = very common (unigrams dominate the head)\n",
        "- right/bottom = rare (longer n-grams shift into the tail)\n",
        "\n",
        "You can check how quickly frequency drops as the rank increase. \n",
        "\n",
        "- Points above the line are over-common for their rank (often set phrases); \n",
        "- Points below the line are under-common relative to the trend.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the first plot, ranks are computed globally across all n-grams and frequencies are normalized by the single global maximum, with one dashed trend line fit to all points. This gives a corpus-wide view and allows direct global rank comparisons, though smaller groups can look compressed under the global scaling. \n",
        "\n",
        "In the second plot, ranks and normalizations are done within each n-gram length, and each group has its own dashed trend line. This makes it easier to compare the slopes and shapes of Unigrams, Bigrams, and Trigrams on fair, per-group scales.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Looking at actual phrases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's see what these frequency distributions actually contain. The histograms showed us the shape - here we'll look at concrete examples. We've divided the frequency spectrum into 8 logarithmic bands (matching our histograms) and sampled actual phrases from each band. Which range seems to contain the n-grams that seems to signal framing?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.mi_visualization import ngram_examples_grid\n",
        "\n",
        "df = session_ngrams\n",
        "\n",
        "table_df, fig = ngram_examples_grid(\n",
        "    df=df,\n",
        "    x_col='frequency_in_session', x_mode='continuous', x_binning='log', x_bins=8,\n",
        "    y_col='ngram_length',   y_mode='categorical', y_binning='none',\n",
        "    examples_per_cell=8, wrap_width=14, figsize=(18, 10),\n",
        "    y_label='n-gram lengths', tick_fontsize=26, label_fontsize=28, cell_text_fontsize=18,\n",
        "    x_label='Frequency (log bins)', \n",
        "    cell_fill='#e9edf2', cell_fill_alpha=0.7\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Speaker adoption of n-grams\n",
        "A phrase used by only one person might be an individual peculiarity. Phrases adopted by multiple team members suggest shared framing. So we calculate how many unique speakers have used each phrase. This will narrow down our candidates significantly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count unique speakers for each n-gram in the corpus. Add speaker_count column to the session_ngrams.\n",
        "session_ngrams = count_speakers_per_ngram(\n",
        "    session_ngrams=session_ngrams,\n",
        "    session_turns=session_turns,\n",
        "    text_column='text'\n",
        ")\n",
        "\n",
        "session_ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create histogram of speaker counts\n",
        "# Most phrases are used by only 1-2 speakers\n",
        "# Very few achieve team-wide adoption\n",
        "fig, ax = plt.subplots(figsize=(6, 8))\n",
        "max_speakers = session_ngrams['speaker_count'].max()\n",
        "bins = range(1, int(max_speakers) + 2)\n",
        "ax.hist(session_ngrams['speaker_count'], bins=bins, edgecolor='black', alpha=0.7)\n",
        "ax.set_xlabel('Number of speakers', fontsize=10) \n",
        "ax.set_ylabel('Number of phrases', fontsize=10)   \n",
        "ax.set_title('Distribution of speaker adoption', fontsize=10)\n",
        "ax.set_xticks(range(1, min(int(max_speakers) + 1, 11)))\n",
        "ax.tick_params(axis='both', which='major', labelsize=8)  # Set x and y tick font size to 8\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As the histogram shows, the vast majority of phrases are used by just one or two speakers, with progressively fewer phrases achieving broader team adoption. This is expected and actually helpful for our analysis. By requiring phrases to be used by multiple speakers (say ≥2 or ≥3), we can dramatically reduce our candidate pool "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Examples accros speaker adoption levels\n",
        "We use the same \"ngram_examples_grid\" function, dividing the frequency spectrum into logarithmic bands and sampling actual phrases from each band."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looking at this grid, we can see clear patterns emerge as we move across frequency ranges and speaker adoption levels.\n",
        "\n",
        "Notice how the grid becomes increasingly sparse as we move right - very few phrases achieve both high frequency AND broad speaker adoption unless they're basic function words. The cells start disappearing around the 6-speaker mark for moderate frequencies, and only the most frequent bins retain phrases at 10+ speakers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.mi_visualization import ngram_examples_grid\n",
        "\n",
        "table_df, fig = ngram_examples_grid(\n",
        "    df=session_ngrams,\n",
        "    x_col='frequency_in_session', x_mode='continuous', x_binning='log', x_bins=8,\n",
        "    y_col='speaker_count',   y_mode='categorical', y_binning='none',\n",
        "    color_col='ngram_length', color_mode='categorical', color_scheme='v',\n",
        "    color_categories=[1, 2, 3],\n",
        "    examples_per_cell=4, wrap_width=20, figsize=(10, 14),\n",
        "    y_label='Speaker count', tick_fontsize=12, label_fontsize=12, cell_text_fontsize=9,\n",
        "    x_label='Frequency (log bins)', \n",
        "    cell_fill='#e9edf2', cell_fill_alpha=0.7\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Finding Cohesive Word Combinations: Calculating Mutual Information Scores\n",
        "\n",
        "\n",
        "Not all word sequences are meaningful phrases. *“status symbol”* refers to a concept; *“status the”* just happened to appear together.  \n",
        "**Mutual Information (PMI)** helps us distinguish between these by measuring whether words appear together **more often than chance would predict**. for words $w_1,w_2$: \n",
        "\n",
        "$$MI(w_1, w_2) = \\log_2 \\frac{P(w_1, w_2)}{P(w_1) \\cdot P(w_2)}$$\n",
        "\n",
        "In practical terms:\n",
        "\n",
        "- **High MI** → words form a cohesive unit (stronger-than-chance association).  \n",
        "- **MI ≈ 0** → words behave independently.  \n",
        "- **Low/negative MI** → co-occurrence is weaker than chance.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from src.mutual_information import analyze_mutual_information\n",
        "\n",
        "# Analyze mutual information for n-grams in the session_turns\n",
        "mi_df = analyze_mutual_information(\n",
        "    df=session_turns,\n",
        "    text_column='text',\n",
        "    n_gram_max=3, # max ngram length to consider\n",
        "    min_frequency=2,\n",
        "    min_length=2,\n",
        "    mi_column='mi_in_session'\n",
        ")\n",
        "\n",
        "# Merge the mi_in_session column with the existing session_ngrams dataframe\n",
        "session_ngrams = session_ngrams.merge(mi_df[['ngram', 'mi_in_session']], on='ngram', how='left')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The n-gram example grid below shows how **Cohesion (PMI)** relates to **how common a phrase is** in the session.\n",
        "\n",
        "**Axes**\n",
        "- **x (right)** → higher **Cohesion (PMI)**: words co-occur more than chance → tighter phrase.\n",
        "- **y (up)** → higher **frequency rank** → **less frequent** phrase (rank 1 = most frequent).  \n",
        "  *(We use log-binned ranks to separate the head from the long tail.)*\n",
        "\n",
        "**How to read the grid**\n",
        "- **Top-right:** cohesive **but rare** → niche/interesting candidates to inspect.\n",
        "- **Bottom-right:** cohesive **and common** → strong, widely-used phrases.\n",
        "- **Top-left:** rare **and** low cohesion → likely noise/accidental adjacency.\n",
        "- **Bottom-left:** common **but** low cohesion → generic function-word combinations.\n",
        "\n",
        "**Filters applied:** `speaker_count > 1`, `frequency_in_session > 3`, `ngram_length > 1`.  \n",
        "**Color:** `ngram_length` (bigrams vs trigrams).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.mi_visualization import ngram_examples_grid\n",
        "\n",
        "# this\n",
        "df = session_ngrams[(session_ngrams['speaker_count'] > 1) & (session_ngrams['frequency_in_session'] > 3) & (session_ngrams['ngram_length'] > 1)]\n",
        "\n",
        "table_df, fig = ngram_examples_grid(\n",
        "    df=df,\n",
        "    x_col='mi_in_session', x_mode='continuous', x_binning='linear', x_bins=6,\n",
        "    y_col='frequency_rank_global',   y_mode='continuous', y_binning='log', y_bins=7,\n",
        "    color_col='ngram_length', color_mode='categorical', color_scheme='tableau20b',    \n",
        "    examples_per_cell=6, wrap_width=22, figsize=(12, 14), tick_fontsize=12, label_fontsize=12, cell_text_fontsize=9,\n",
        "    example_order_col = \"frequency_rank_global\",\n",
        "    cell_fill='#e9edf2', cell_fill_alpha=0.7\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Normalising MI scores by n-gram length to make them comparable\n",
        "\n",
        "Longer n-grams naturally have different MI distributions than shorter ones. A three-word phrase has more opportunity to be \"surprising\" than a two-word phrase, simply because it involves more words whose co-occurrence could be unexpected.\n",
        "\n",
        "We solve this by Z-score normalization that converts each MI score to standard deviations from the mean of its length group:\n",
        "\n",
        "$$z = \\frac{MI - \\text{mean}_{mi}}{\\text{std}_{mi}}$$\n",
        "\n",
        "We apply following steps:\n",
        "\n",
        "- Calculate mean and std for bigrams and trigrams separately\n",
        "- Convert each MI to z-score: (MI - mean) / std\n",
        "- Now both distributions are centered and comparable\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.mi_visualization import normalise_values\n",
        "\n",
        "# Z-score MI per n-gram length (common choice)\n",
        "session_ngrams = normalise_values(\n",
        "    session_ngrams,\n",
        "    freq_col='mi_in_session',\n",
        "    out_col='mi_in_session_z',\n",
        "    method='zscore',\n",
        "    group_by='ngram_length'\n",
        ")\n",
        "\n",
        "session_ngrams = normalise_values(\n",
        "    session_ngrams,\n",
        "    freq_col='frequency_in_session',\n",
        "    out_col='frequency_z_within_length',\n",
        "    method='zscore',\n",
        "    group_by='ngram_length'\n",
        ")\n",
        "\n",
        "session_ngrams = normalise_values(\n",
        "    session_ngrams,\n",
        "    freq_col='frequency_in_session',\n",
        "    out_col='frequency_z_global',\n",
        "    method='zscore',\n",
        "    group_by=None\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.mi_visualization import plot_mi_histograms_before_after\n",
        "\n",
        "fig, axes = plot_mi_histograms_before_after(\n",
        "    df=session_ngrams,\n",
        "    length_col='ngram_length',\n",
        "    lengths=(2,3),\n",
        "    mi_col='mi_in_session',\n",
        "    mi_z_col='mi_in_session_z',\n",
        "    bins=30,\n",
        "    density=True,\n",
        "    alpha=0.6,\n",
        "    figsize=(12,4)\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Distributions: raw PMI vs normalised PMI (PMI z)**\n",
        "\n",
        "- **Before** (raw `mi_in_session`): trigrams tend to sit higher than bigrams due to length effects.\n",
        "- **After** (`mi_in_session_z`): both lengths are on a **shared scale**.\n",
        "  - **z = 0**: Average association for that length\n",
        "  - **z > 2**: Unusually strong association\n",
        "  - **z < -2**: Likely random co-occurrence\n",
        "\n",
        "This confirms that z-scoring by length makes cohesion comparable across bigrams/trigrams."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Example ngrams across different MI values\n",
        "Let's see what high and low MI phrases actually look like:\n",
        "\n",
        "- **x (→ right)**: `mi_in_session_z` — **Cohesion (PMI z)** within length.\n",
        "- **y (↑ up)**: `frequency_rank_within_length` (log-binned) — **less frequent** as you go up.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**What these MI ranges mean**\n",
        "\n",
        "- Very Low (z < -2): Random adjacencies:\n",
        "  - \"and the\", \"it was\", \"to be\": Words that just happened to be next to each other\n",
        "- Low to Average (-2 to 0): Weak associations\n",
        "  - Common function word combinations\n",
        "  - No special conceptual bond\n",
        "- Above Average to High (0 to 2): Meaningful combinations\n",
        "  - \"makes sense\", \"opportunity areas\"\n",
        "  - Words that preferentially occur together\n",
        "- Very High (z > 2): Strong conceptual units\n",
        "  - \"status symbol\", \"sexy commitment\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.mi_visualization import ngram_examples_grid\n",
        "\n",
        "df = session_ngrams[\n",
        "   # (session_ngrams['mi_in_session_z'] > 0) & \n",
        "    (session_ngrams['speaker_count'] > 2) & \n",
        "    (session_ngrams['frequency_in_session'] > 3) & \n",
        "    (session_ngrams['ngram_length'] > 1)]\n",
        "\n",
        "table_df, fig = ngram_examples_grid(\n",
        "    df=df,\n",
        "    x_col='mi_in_session_z', x_mode='continuous', x_binning='linear', x_bins=6,\n",
        "    y_col='frequency_rank_within_length',   y_mode='continuous', y_binning='log', y_bins=10,\n",
        "    color_col='ngram_length', color_mode='categorical', color_scheme='tableau20b',    \n",
        "    examples_per_cell=6, wrap_width=20, figsize=(12, 16),\n",
        "    y_label='Frequency ranks of ngrams within same length (log bins)', tick_fontsize=12, label_fontsize=12, cell_text_fontsize=9,\n",
        "    x_label='Normalised MI (linear bins)', \n",
        "    cell_fill='#e9edf2', cell_fill_alpha=0.7\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Narrowing down the range of MI and frequency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We filter by 'normalised mi > 0' and 'frequency_rank > 450 since there seems to be no candidates here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.mi_visualization import ngram_examples_grid\n",
        "\n",
        "df = session_ngrams[\n",
        "    (session_ngrams['mi_in_session_z'] > 0.5) & \n",
        "    (session_ngrams['speaker_count'] > 1) & \n",
        "    (session_ngrams['frequency_in_session'] > 3) & \n",
        "    (session_ngrams['ngram_length'] > 1)]\n",
        "\n",
        "table_df, fig = ngram_examples_grid(\n",
        "    df=df,\n",
        "    x_col='mi_in_session_z', x_mode='continuous', x_binning='linear', x_bins=6,\n",
        "    y_col='frequency_rank_within_length',   y_mode='continuous', y_binning='log', y_bins=10,\n",
        "    color_col='ngram_length', color_mode='categorical', color_scheme='tableau20b',    \n",
        "    examples_per_cell=6, wrap_width=22, figsize=(12, 16),\n",
        "    y_label='Frequency ranks of ngrams within same length (log bins)', tick_fontsize=12, label_fontsize=12, cell_text_fontsize=9,\n",
        "    x_label='Normalised MI (linear bins)', \n",
        "    cell_fill='#e9edf2', cell_fill_alpha=0.7\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Save outputs to use in the next notebook\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save unified session_ngrams with agreed naming for downstream notebooks\n",
        "session_ngrams.to_csv('outputs/session_ngrams_1.csv', index=False)\n",
        "session_turns.to_csv('outputs/session_turns.csv', index=False)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "vis-analysis",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.23"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
